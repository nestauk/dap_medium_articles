{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83467139",
   "metadata": {},
   "source": [
    "Welcome to the code behind Nesta's medium article, \"Training spaCy's spancat component in Python\"! üëã\n",
    "\n",
    "Although spaCy pushes for config-based training, sometimes you simply want to experiment with a component in a jupyter notebook without the headache (and commitment) of setting everything up. And although there is plenty online on how to train a custom NER model in spaCy, there is virtually nothing on how to do the same for a custom spancat model.\n",
    "\n",
    "Training a custom spancat model looks very similar to training a custom NER model but has several key differences including:\n",
    "\n",
    "1. the training data needs to be in a different format; \n",
    "2. you must convert your training data to a list of spaCy Examples to train the model;\n",
    "3. you access the spans and span-level confidence scores differently.\n",
    "\n",
    "This notebook will walk you through training a custom spancat model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4454f467",
   "metadata": {},
   "source": [
    "#### 0. Load Libraries and parameters\n",
    "let's first import the libraries and parameters we will need to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "162d1d6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T09:36:35.646313Z",
     "iopub.status.busy": "2022-10-19T09:36:35.645829Z",
     "iopub.status.idle": "2022-10-19T09:36:36.683569Z",
     "shell.execute_reply": "2022-10-19T09:36:36.683098Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "from spacy import displacy\n",
    "\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import Example\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from spacy.pipeline.spancat import DEFAULT_SPANCAT_MODEL\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b71ab98",
   "metadata": {},
   "source": [
    "we're going to instantiate a blank spaCy object and name the key to store our labelled spans \"sc\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed27298",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T09:36:36.686143Z",
     "iopub.status.busy": "2022-10-19T09:36:36.685940Z",
     "iopub.status.idle": "2022-10-19T09:36:36.820046Z",
     "shell.execute_reply": "2022-10-19T09:36:36.819642Z"
    }
   },
   "outputs": [],
   "source": [
    "#instantiate blank spaCy object\n",
    "nlp = spacy.blank('en')\n",
    "#define your span key name\n",
    "span_key = \"sc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702fef01",
   "metadata": {},
   "source": [
    "#### 1. Load Data\n",
    "Once we've loaded the relevant libraries and parameters, lets download and clean up our data. The README.md tells you how to download the data üíæ.\n",
    "\n",
    "We want to extract people, organisation and locations tags. So, we can just replace the other tags in the data we don't care about with 'O'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "791cdc32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T09:36:36.822485Z",
     "iopub.status.busy": "2022-10-19T09:36:36.822325Z",
     "iopub.status.idle": "2022-10-19T09:36:38.987312Z",
     "shell.execute_reply": "2022-10-19T09:36:38.986827Z"
    }
   },
   "outputs": [],
   "source": [
    "#load the data and fill nans with sentence #s\n",
    "ner_data = (pd.read_csv(\"data.csv\", encoding='ISO-8859-1')\n",
    "            .fillna(method='ffill'))\n",
    "\n",
    "#replace tags we don't care about with 'O'\n",
    "ents_to_replace = ['tim', 'gpe', 'art', 'eve', 'nat']\n",
    "bad_ents = []\n",
    "for ent in list(set(ner_data.Tag)):\n",
    "    if any(ent.endswith(e) for e in ents_to_replace):\n",
    "        bad_ents.append(ent)\n",
    "        \n",
    "ner_data = ner_data.replace(bad_ents, 'O')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c5349a",
   "metadata": {},
   "source": [
    "#### 2. Format data for spaCy\n",
    "Now that we've loaded and replaced the non-relevant tags with 'O', let's format the data in a way that spaCy can handle. We will need to get the start and end of spans in the text. \n",
    "\n",
    "We will do this with the get_span_indx function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9956f6db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T09:36:38.989936Z",
     "iopub.status.busy": "2022-10-19T09:36:38.989657Z",
     "iopub.status.idle": "2022-10-19T09:36:38.995300Z",
     "shell.execute_reply": "2022-10-19T09:36:38.994961Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_span_indx(\n",
    "    labels: List[str],\n",
    "    words: List[str],\n",
    "    sentence: str\n",
    ") -> List[tuple]:\n",
    "    \"\"\"Gets span starts and ends for Spacy spancat component.\n",
    "        \n",
    "        Returns list of tuples where the first element of the \n",
    "        tuple is the span start, the second element of the tuple\n",
    "        is the span end and the third element of the tuple is\n",
    "        the span category. \n",
    "    \"\"\"\n",
    "    #gets list of indices corresponding to labelled words \n",
    "    label_indx = []\n",
    "    temp_list = []\n",
    "\n",
    "    for i, l in enumerate(labels):\n",
    "        if l != 'O':\n",
    "            temp_list.append(i)\n",
    "        else:\n",
    "            label_indx.append(temp_list)\n",
    "            temp_list = []    \n",
    "        if i == len(labels) - 1:\n",
    "            label_indx.append(temp_list)\n",
    "\n",
    "    clean_label_indx = [x for x in label_indx if len(x) > 0]\n",
    "\n",
    "    spans = []\n",
    "    for indx in clean_label_indx:\n",
    "        if len(indx) == 1:\n",
    "            span = words[indx[0]]\n",
    "            label = labels[indx[0]].split('-')[-1].upper()\n",
    "        else:\n",
    "            span = ' '.join([words[i] for i in indx])  \n",
    "            label = [labels[i].split('-')[-1].upper() for i in indx][0]\n",
    "        #remove punctuation and strip whitespace for both sents and spans\n",
    "        span_clean = span.translate(str.maketrans('', '', string.punctuation))\n",
    "        for m in re.finditer(span_clean, sentence):\n",
    "            spans.append((m.start(), m.end(), label))\n",
    "    \n",
    "    return spans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a59f7f9",
   "metadata": {},
   "source": [
    "Once we've written a function to extract span start and end indices, we will format the training data for spaCy such that:\n",
    "\n",
    "```\n",
    "  ('Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country',\n",
    "  {'spans': {'sc': [(48, 54, 'GEO'), (77, 81, 'GEO')]}})\n",
    "```\n",
    "\n",
    "Where the first element of the tuple is the text and the second element of the tuple is a nested dictionary where the values of the span_key (in this instance, \"sc\") is a list of tuples containing the span character start, span character end and span category. \n",
    "\n",
    "Finally, we convert this data structure into a spaCy Example object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25d40a05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T09:36:38.997752Z",
     "iopub.status.busy": "2022-10-19T09:36:38.997598Z",
     "iopub.status.idle": "2022-10-19T09:36:48.597473Z",
     "shell.execute_reply": "2022-10-19T09:36:48.597087Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create spaCy compliant training data \n",
    "train_data = []\n",
    "for sent, sent_info in ner_data.groupby(\"Sentence #\"):\n",
    "    words = list(sent_info[\"Word\"])\n",
    "    #convert words to sentence and get rid of spaces between punctuation characters\n",
    "    sentence = re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', \" \".join(words))\n",
    "    #get labels\n",
    "    labels = list(sent_info['Tag'])\n",
    "    #identify token span start, span ends and span category\n",
    "    span_ents = get_span_indx(labels, words, sentence)\n",
    "    #create spaCy compliant spans[span_key] dict\n",
    "    annotation = {'spans':{span_key: span_ents}}    \n",
    "    #convert sentence and annotation into spaCy examples\n",
    "    train_data.append(Example.from_dict(nlp.make_doc(sentence), annotation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51446a0a",
   "metadata": {},
   "source": [
    "#### 3. Train spancat component with labelled entities\n",
    "\n",
    "Now that we've formatted the training data in such a way to be able to train the spancat component, let's instantiate a blank spaCy object and add our labels to the component üè∑Ô∏è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0429baf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T09:36:48.599969Z",
     "iopub.status.busy": "2022-10-19T09:36:48.599823Z",
     "iopub.status.idle": "2022-10-19T09:36:48.621966Z",
     "shell.execute_reply": "2022-10-19T09:36:48.621561Z"
    }
   },
   "outputs": [],
   "source": [
    "#spancat config \n",
    "config = {\n",
    "    #this refers to the minimum probability to consider a prediction positive\n",
    "    \"threshold\": 0.5,\n",
    "    #the span key refers to the key in doc.spans \n",
    "    \"spans_key\": span_key,\n",
    "    #this refers to the maximum number of labels to consider positive per span\n",
    "    \"max_positive\": None,\n",
    "     #a model instance that is given a list of documents with start end indices representing the labelled spans\n",
    "    \"model\": DEFAULT_SPANCAT_MODEL,\n",
    "    #A function that suggests spans. This suggester is fixed n-gram length of up to 3 tokens\n",
    "    \"suggester\": {\"@misc\": \"spacy.ngram_suggester.v1\", \"sizes\": [1, 2, 3]},\n",
    "}\n",
    "#add spancat component to nlp object\n",
    "nlp.add_pipe(\"spancat\", config=config)\n",
    "#get spancat component \n",
    "span=nlp.get_pipe('spancat')\n",
    "\n",
    "#Add labels to spancat component \n",
    "for label in ('GEO', 'PER', 'ORG'):\n",
    "    span.add_label(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde47ed9",
   "metadata": {},
   "source": [
    "Now that we've done that, let's get the pipe we want to train on, initialise the spacy object and create an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97c82869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T09:36:48.624320Z",
     "iopub.status.busy": "2022-10-19T09:36:48.624168Z",
     "iopub.status.idle": "2022-10-19T09:36:48.650608Z",
     "shell.execute_reply": "2022-10-19T09:36:48.650212Z"
    }
   },
   "outputs": [],
   "source": [
    "#get pipe you want to train on \n",
    "pipe_exceptions = [\"spancat\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# initialise spacy object \n",
    "nlp.initialize()\n",
    "sgd = nlp.create_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70008ad9",
   "metadata": {},
   "source": [
    "...and let the training begin! ‚è≤Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c940c55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T09:36:48.653091Z",
     "iopub.status.busy": "2022-10-19T09:36:48.652950Z",
     "iopub.status.idle": "2022-10-19T10:14:13.418526Z",
     "shell.execute_reply": "2022-10-19T10:14:13.417992Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñé                                      | 1/10 [03:40<33:02, 220.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 Losses: {'spancat': 21944.50796819647}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 2/10 [07:27<29:56, 224.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 Losses: {'spancat': 16376.95999833569}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 3/10 [11:09<26:01, 223.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 Losses: {'spancat': 14718.516249835026}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 4/10 [14:50<22:13, 222.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 Losses: {'spancat': 13677.246304473534}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 5/10 [18:30<18:27, 221.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 Losses: {'spancat': 12934.318437874244}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 6/10 [22:11<14:45, 221.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 Losses: {'spancat': 12232.822345284116}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 7/10 [25:57<11:08, 222.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 Losses: {'spancat': 11644.008111855655}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 8/10 [29:45<07:28, 224.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 Losses: {'spancat': 11169.782576366852}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/10 [33:32<03:45, 225.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 Losses: {'spancat': 10808.241575563407}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [37:24<00:00, 224.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 Losses: {'spancat': 10357.203760585631}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#start training the spancat component \n",
    "all_losses = []\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "    for iteration in tqdm(range(10)):\n",
    "        # shuffling examples before every iteration\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            nlp.update(list(batch), losses=losses, drop=0.1, sgd=sgd)\n",
    "        print(\"epoch: {} Losses: {}\".format(iteration, str(losses)))\n",
    "        all_losses.append(losses['spancat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01896b21",
   "metadata": {},
   "source": [
    "**Note:** we are only training for 10 iterations to save time, which likely won't yield great results. A higher number of iterations like 30 would be better.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a090ac6",
   "metadata": {},
   "source": [
    "We can investigate the losses by printing all_losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c72febb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T10:14:13.422345Z",
     "iopub.status.busy": "2022-10-19T10:14:13.422167Z",
     "iopub.status.idle": "2022-10-19T10:14:13.425024Z",
     "shell.execute_reply": "2022-10-19T10:14:13.424521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21944.50796819647\n",
      "16376.95999833569\n",
      "14718.516249835026\n",
      "13677.246304473534\n",
      "12934.318437874244\n",
      "12232.822345284116\n",
      "11644.008111855655\n",
      "11169.782576366852\n",
      "10808.241575563407\n",
      "10357.203760585631\n"
     ]
    }
   ],
   "source": [
    "for loss in all_losses:\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b578e",
   "metadata": {},
   "source": [
    "#### 4. Apply the custom trained spancat model \n",
    "\n",
    "Now that we've trained our model, lets apply it to a sentence in yesterday's news article on Lizz truss and see what spans it predicts as people, organisations and locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "409c4018",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T10:14:13.428079Z",
     "iopub.status.busy": "2022-10-19T10:14:13.427907Z",
     "iopub.status.idle": "2022-10-19T10:14:13.436827Z",
     "shell.execute_reply": "2022-10-19T10:14:13.436199Z"
    }
   },
   "outputs": [],
   "source": [
    "# Testing the model on sentence from the BBC news article yesterday\n",
    "text = \"Yes, Liz Truss would deliver the governments 2050 net zero target in the United Kingdom, but the prime minister also restated her determination to issue more oil and gas licences in the North Sea.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987796de",
   "metadata": {},
   "source": [
    "We're able to print the predicted spans and span-level confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91b4fbcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T10:14:13.440175Z",
     "iopub.status.busy": "2022-10-19T10:14:13.439943Z",
     "iopub.status.idle": "2022-10-19T10:14:13.443223Z",
     "shell.execute_reply": "2022-10-19T10:14:13.442790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER 0.9257805\n",
      "GEO 0.9826538\n",
      "GEO 0.98000926\n"
     ]
    }
   ],
   "source": [
    "#print predicted spans and entity level scores\n",
    "spans = doc.spans[span_key]\n",
    "for span, confidence in zip(spans, spans.attrs[\"scores\"]):\n",
    "    print(span.label_, confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa8c1e6",
   "metadata": {},
   "source": [
    "Finally, we can use spaCy's `displacy` to show the predicted spans in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d60887c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T10:14:13.445570Z",
     "iopub.status.busy": "2022-10-19T10:14:13.445421Z",
     "iopub.status.idle": "2022-10-19T10:14:13.452906Z",
     "shell.execute_reply": "2022-10-19T10:14:13.452457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"spans\" style=\"line-height: 2.5; direction: ltr\">Yes , \n",
       "<span style=\"font-weight: bold; display: inline-block; position: relative; height: 60px;\">\n",
       "    Liz\n",
       "    \n",
       "<span style=\"background: #ddd; top: 40px; height: 4px; left: -1px; width: calc(100% + 2px); position: absolute;\">\n",
       "</span>\n",
       "\n",
       "    \n",
       "<span style=\"background: #ddd; top: 40px; height: 4px; border-top-left-radius: 3px; border-bottom-left-radius: 3px; left: -1px; width: calc(100% + 2px); position: absolute;\">\n",
       "    <span style=\"background: #ddd; z-index: 10; color: #000; top: -0.5em; padding: 2px 3px; position: absolute; font-size: 0.6em; font-weight: bold; line-height: 1; border-radius: 3px\">\n",
       "        PER\n",
       "    </span>\n",
       "</span>\n",
       "\n",
       "\n",
       "</span>\n",
       "\n",
       "<span style=\"font-weight: bold; display: inline-block; position: relative; height: 60px;\">\n",
       "    Truss\n",
       "    \n",
       "<span style=\"background: #ddd; top: 40px; height: 4px; left: -1px; width: calc(100% + 2px); position: absolute;\">\n",
       "</span>\n",
       "\n",
       "    \n",
       "</span>\n",
       "would deliver the governments 2050 net zero target in the \n",
       "<span style=\"font-weight: bold; display: inline-block; position: relative; height: 60px;\">\n",
       "    United\n",
       "    \n",
       "<span style=\"background: #ddd; top: 40px; height: 4px; left: -1px; width: calc(100% + 2px); position: absolute;\">\n",
       "</span>\n",
       "\n",
       "    \n",
       "<span style=\"background: #ddd; top: 40px; height: 4px; border-top-left-radius: 3px; border-bottom-left-radius: 3px; left: -1px; width: calc(100% + 2px); position: absolute;\">\n",
       "    <span style=\"background: #ddd; z-index: 10; color: #000; top: -0.5em; padding: 2px 3px; position: absolute; font-size: 0.6em; font-weight: bold; line-height: 1; border-radius: 3px\">\n",
       "        GEO\n",
       "    </span>\n",
       "</span>\n",
       "\n",
       "\n",
       "</span>\n",
       "\n",
       "<span style=\"font-weight: bold; display: inline-block; position: relative; height: 60px;\">\n",
       "    Kingdom\n",
       "    \n",
       "<span style=\"background: #ddd; top: 40px; height: 4px; left: -1px; width: calc(100% + 2px); position: absolute;\">\n",
       "</span>\n",
       "\n",
       "    \n",
       "</span>\n",
       ", but the prime minister also restated her determination to issue more oil and gas licences in the \n",
       "<span style=\"font-weight: bold; display: inline-block; position: relative; height: 60px;\">\n",
       "    North\n",
       "    \n",
       "<span style=\"background: #ddd; top: 40px; height: 4px; left: -1px; width: calc(100% + 2px); position: absolute;\">\n",
       "</span>\n",
       "\n",
       "    \n",
       "<span style=\"background: #ddd; top: 40px; height: 4px; border-top-left-radius: 3px; border-bottom-left-radius: 3px; left: -1px; width: calc(100% + 2px); position: absolute;\">\n",
       "    <span style=\"background: #ddd; z-index: 10; color: #000; top: -0.5em; padding: 2px 3px; position: absolute; font-size: 0.6em; font-weight: bold; line-height: 1; border-radius: 3px\">\n",
       "        GEO\n",
       "    </span>\n",
       "</span>\n",
       "\n",
       "\n",
       "</span>\n",
       "\n",
       "<span style=\"font-weight: bold; display: inline-block; position: relative; height: 60px;\">\n",
       "    Sea\n",
       "    \n",
       "<span style=\"background: #ddd; top: 40px; height: 4px; left: -1px; width: calc(100% + 2px); position: absolute;\">\n",
       "</span>\n",
       "\n",
       "    \n",
       "</span>\n",
       ". </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show predicted spans \n",
    "displacy.render(doc, style=\"span\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47b333a",
   "metadata": {},
   "source": [
    "And that's that! If you'd like to learn more about the spancat architecture üèóÔ∏è and a bit more detail about the code, do refer back to the medium article. This is purely demonstrative so we're not training the model for a large # of iterations nor evaluating the model, tuning hyperparameters, playing around with the suggester or using a spans-specific dataset.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ojd_daps_skills",
   "language": "python",
   "name": "ojd_daps_skills"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
