{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395daaf6",
   "metadata": {},
   "source": [
    "*Hello again!* ðŸ‘‹\n",
    "\n",
    "This notebook is the <u>second (and last)</u> part of a **tutorial** on how to **get started with Twitter API v2 using Python** ðŸ¤“! Read our medium blog post [here](https://medium.com/data-analytics-at-nesta).\n",
    "\n",
    "In this notebook, we make use of the **recent search** endpoint to collect Twitter data on heat pumps and gas boilers from the last 7 days.\n",
    "\n",
    "**More on the use case**\n",
    "\n",
    "The [sustainable future mission](https://www.nesta.org.uk/sustainable-future/) at [Nesta](https://www.nesta.org.uk/) is focused on projects to help decarbonise UK homes, with special interest in greener heating systems such as heat pumps. For those who are not familiar with the concept, a heat pump is a low-carbon heating system that captures heat from outside and moves it into your home.\n",
    "\n",
    "What is the sentiment on heat pumps *versus* gas boilers? Have peoples' opinions towards heat pumps changed with time? Which types of users mention heat pumps on Twitter? These are all questions we can answer once we start analysing Twitter data on these topics.\n",
    "\n",
    "But first... Let us have a go at collecting tweets mentioning heat pumps or gas boilers in the past 7 days!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e44eb4",
   "metadata": {},
   "source": [
    "### Importing packages and loading credentials\n",
    "We start by importing the necessary packages to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d9af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d638597b",
   "metadata": {},
   "source": [
    "We import our *bearer_token* which we previously defined as an environment variable. This way you do not have to expose your credentials in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fbb293",
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token = os.environ.get(\"BEARER_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c239cd51",
   "metadata": {},
   "source": [
    "### Preparing our API request\n",
    "We will use the recent search endpoint to collect our first set of tweets. To do that we need to define the endpoint URL, the rules clarifying the data we want to collect and other query parameters such as fields to include and maximum number of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3275464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_url = \"https://api.twitter.com/2/tweets/search/recent\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5cb5cd",
   "metadata": {},
   "source": [
    "We define the following two rules:\n",
    "- tweets matching one of the expressions \"heat pump\"/\"heat pumps\", written in english, which are not retweets;\n",
    "- tweets matching one of the expressions \"gas boiler\"/\"gas boilers\", written in english, which are not retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7e2794",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = [\n",
    "    {\"value\": '(\"heat pump\" OR \"heat pumps\") -is:retweet lang:en', \"tag\": \"heat_pump\"},\n",
    "    {\"value\": '(\"gas boiler\" OR \"gas boilers\") -is:retweet lang:en', \"tag\": \"gas_boiler\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d1ad5",
   "metadata": {},
   "source": [
    "We create a dictionary with query parameters, where we pass the following fields:\n",
    "- **tweet.fields**: fields in the tweet object for which we want to collect information, in this example: the tweet unique identifier, the tweet text, the identifier of the user posting the tweet and the date/time the tweet was created;\n",
    "- **user.fields**: fields in the user object for which we want to collect information, in this example: the user unique identifier, name, username, date/time the user created their account, description, user defined location and whether the user is verified or not;\n",
    "- **expansions**: expansion query parameter with info relating to the user. We need to add this in order to receive user data in our response object.\n",
    "- **max_results**: the maximum number of tweets to be retrieved per request to the API, in this case 100 (which is also the maximum allowed).\n",
    "\n",
    "Unlike our previous example, here we do not define the query rules straight away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dceb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_parameters = {\n",
    "    \"tweet.fields\": \"id,text,author_id,created_at\",\n",
    "    \"user.fields\": \"id,name,username,created_at,description,location,verified\",\n",
    "    \"expansions\": \"author_id\",\n",
    "    \"max_results\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb999b47",
   "metadata": {},
   "source": [
    "### Authentication\n",
    "Authentication is done by bearer token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e1d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_headers(bearer_token: str) -> dict:\n",
    "    \"\"\"\n",
    "    Set up the request headers. \n",
    "    Returns a dictionary summarising the bearer token authentication details.\n",
    "    \"\"\"\n",
    "    return {\"Authorization\": \"Bearer {}\".format(bearer_token)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d335182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = request_headers(bearer_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5de79d",
   "metadata": {},
   "source": [
    "### Connecting to endpoint and taking a look at the data\n",
    "We connect to the endpoint and retrieve our first page of data to see what changed in comparison to the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a779cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_endpoint(endpoint_url: str, headers: dict, parameters: dict) -> json:\n",
    "    \"\"\"\n",
    "    Connects to the endpoint and requests data.\n",
    "    Returns a json with Twitter data if a 200 status code is yielded.\n",
    "    Programme stops if there is a problem with the request and sleeps\n",
    "    if there is a temporary problem accessing the endpoint.\n",
    "    \"\"\"\n",
    "    response = requests.request(\n",
    "        \"GET\", url=endpoint_url, headers=headers, params=parameters\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        if response.status_code >= 400 and response.status_code < 500:\n",
    "            raise Exception(\n",
    "                \"Cannot get data, the program will stop!\\nHTTP {}: {}\".format(\n",
    "                    response.status_code, response.text\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        sleep_seconds = random.randint(5, 60)\n",
    "        print(\n",
    "            \"Cannot get data, your program will sleep for {} seconds...\\nHTTP {}: {}\".format(\n",
    "                sleep_seconds, response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "        time.sleep(sleep_seconds)\n",
    "        return connect_to_endpoint(endpoint_url, headers, parameters)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643757a3",
   "metadata": {},
   "source": [
    "Let us retrieve the first page of tweets for our first rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c08b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_parameters[\"query\"] = rules[0][\"value\"]\n",
    "json_response = connect_to_endpoint(endpoint_url, headers, query_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51816d01",
   "metadata": {},
   "source": [
    "Now the json_response dictionary contains 3 keys: *data*, *includes* and *meta*. The only difference from the previous example is the *includes* field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac49c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57426735",
   "metadata": {},
   "source": [
    "json_response[\"includes\"] is also a dictionary and it contains one key, \"users\", because we are now also collecting user information. If other information such as places/location information was also being collected, then we would have another key in our json_response[\"includes\"] dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb5f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response[\"includes\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f517b8a",
   "metadata": {},
   "source": [
    "This is what each user dictionary looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response[\"includes\"][\"users\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4e0ed4",
   "metadata": {},
   "source": [
    "### Collecting tweets from the past 7 days\n",
    "\n",
    "We define a functions to process twitter data and we start the data collection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7283e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_twitter_data(\n",
    "    json_response: json,\n",
    "    query_tag: str,\n",
    "    tweets_data: pd.DataFrame,\n",
    "    users_data: pd.DataFrame,\n",
    ") -> (pd.DataFrame, pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Adds new tweet/user information to the table of\n",
    "    tweets/users and saves dataframes as pickle files,\n",
    "    if data is avaiable.\n",
    "    \n",
    "    Returns the tweets and users updated dataframes.\n",
    "    \"\"\"\n",
    "    if \"data\" in json_response.keys():\n",
    "        new = pd.DataFrame(json_response[\"data\"])\n",
    "        tweets_data = pd.concat([tweets_data, new])\n",
    "        tweets_data.to_pickle(\"tweets_\" + query_tag + \".pkl\")\n",
    "\n",
    "        if \"users\" in json_response[\"includes\"].keys():\n",
    "            new = pd.DataFrame(json_response[\"includes\"][\"users\"])\n",
    "            users_data = pd.concat([users_data, new])\n",
    "            users_data.drop_duplicates(\"id\", inplace=True)\n",
    "            users_data.to_pickle(\"users_\" + query_tag + \".pkl\")\n",
    "\n",
    "    return tweets_data, users_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8856c798",
   "metadata": {},
   "source": [
    "Now that we know what the data looks like, let's start our data collection process!\n",
    "\n",
    "**The data collection process:**\n",
    "- We define empty dataframes where we will store information about tweets and users;\n",
    "- The for loop allows you to go through all your rules;\n",
    "- We update the query parameters query field according to the rule in question;\n",
    "- We connect to the endpoint as in the previous example and process the data, using the process_twitter_data() function;\n",
    "- Then the program sleeps for 5 seconds. This is necessary not to surpass the rate limit. For this specific endpoint and Essential access level, the rate limit is 180 requests/15 minutes per user, which translates into 1 request every 5 seconds so we need to wait for at least 5 seconds before we make another request.\n",
    "- If json_response['meta'] has a next_token (the pagination token) field then it means that we have not reached the final page of tweets, so we add it as a query parameter and collect more tweets;\n",
    "- We repeat the process until  json_response['meta'] no longer contains  next_token field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d11decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = pd.DataFrame()\n",
    "users_data = pd.DataFrame()\n",
    "\n",
    "for i in range(len(rules)):\n",
    "    query_parameters[\"query\"] = rules[i][\"value\"]\n",
    "    query_tag = rules[i][\"tag\"]\n",
    "\n",
    "    json_response = connect_to_endpoint(endpoint_url, headers, query_parameters)\n",
    "    tweets_data, users_data = process_twitter_data(\n",
    "        json_response, query_tag, tweets_data, users_data\n",
    "    )\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    while \"next_token\" in json_response[\"meta\"]:\n",
    "        query_parameters[\"next_token\"] = json_response[\"meta\"][\"next_token\"]\n",
    "\n",
    "        json_response = connect_to_endpoint(endpoint_url, headers, query_parameters)\n",
    "        tweets_data, users_data = process_twitter_data(\n",
    "            json_response, query_tag, tweets_data, users_data\n",
    "        )\n",
    "\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47963601",
   "metadata": {},
   "source": [
    "**We have reached the end of this tutorial on how to collect Twitter data from the past 7 days** ðŸ’ªðŸ¤“\n",
    "\n",
    "This code was inspired in official Twitter code in this [GitHub repo](https://github.com/twitterdev/Twitter-API-v2-sample-code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010828a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter_api_tutorial",
   "language": "python",
   "name": "twitter_api_tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
